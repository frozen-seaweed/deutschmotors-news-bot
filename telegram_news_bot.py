# telegram_news_bot.py
# âœ… ê·¸ëŒ€ë¡œ íŒŒì¼ ì „ì²´ë¥¼ ì´ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”.

import os
import re
import requests
from datetime import datetime, timedelta
from storage import (
    build_keyword_weights,
    load_sent_articles,
    save_sent_articles,
    normalize_url,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
NEWS_API_KEY = os.getenv("NEWS_API_KEY")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë™ì  ì¿¼ë¦¬ ìƒì„± + ë‰´ìŠ¤ ìˆ˜ì§‘
def get_news():
    """
    ì¢‹ì•„ìš” ìƒìœ„ í‚¤ì›Œë“œë¡œ ë™ì  që¥¼ ë§Œë“¤ì–´ NewsAPIì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘.
    - liked_keywords ìƒìœ„ 6ê°œë¥¼ ORë¡œ ë¬¶ì–´ q ìƒì„±
    - ë°ì´í„°ê°€ ì ìœ¼ë©´ ê¸°ë³¸ që¡œ í´ë°±
    - ìµœê·¼ ê¸°ì‚¬ ìœ„ì£¼ë¡œ 36ì‹œê°„ ë²”ìœ„(from=) ì ìš©
    - ë¡œê·¸ í•œ ì¤„ ë‚¨ê¹€: q / fetched / total / since
    """
    # 1) ê¸°ë³¸ q (í´ë°±ìš©)
    base_q = "ìë™ì°¨ OR í˜„ëŒ€ì°¨ OR EV OR ë°°í„°ë¦¬ OR ëª¨ë¹Œë¦¬í‹° OR ê¸°ì•„"

    # 2) ì„ í˜¸ í‚¤ì›Œë“œ ìƒìœ„ N ì¶”ì¶œ
    try:
        weights = build_keyword_weights() or {}
    except Exception:
        weights = {}

    topK = []
    if isinstance(weights, dict):
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ìƒìœ„ 6ê°œ
        topK = sorted(weights.items(), key=lambda x: x[1], reverse=True)[:6]

    liked_terms = []

    # ê°„ë‹¨ ë™ì˜ì–´/ì •ê·œí™” (í•„ìš” ìµœì†Œë§Œ)
    syn = {
        "ev": "ì „ê¸°ì°¨",
        "ë²¤ì¸ ": "ë©”ë¥´ì„¸ë°ìŠ¤",
    }

    for kw, _score in topK:
        if not kw:
            continue
        k = str(kw).strip()
        if not k:
            continue
        # ë™ì˜ì–´ ë§¤í•‘
        lk = syn.get(k.lower(), k)
        # ë©€í‹°ì›Œë“œëŠ” ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
        if " " in lk:
            liked_terms.append(f"\"{lk}\"")
        else:
            liked_terms.append(lk)

    # 3) ìµœì¢… q êµ¬ì„± (ì„ í˜¸ ìˆìœ¼ë©´ ì„ í˜¸ OR ê¸°ë³¸, ì—†ìœ¼ë©´ ê¸°ë³¸ q)
    if liked_terms:
        q = f"({' OR '.join(liked_terms)}) OR ({base_q})"
    else:
        q = base_q

    # 4) ìµœê·¼ì„± í•„í„°: ì§€ë‚œ 36ì‹œê°„(UTC)
    since = (datetime.utcnow() - timedelta(hours=36)).isoformat(timespec="seconds") + "Z"

    # 5) NewsAPI í˜¸ì¶œ
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": q,
        "language": "ko",
        "pageSize": 20,            # í•„ìš” ì‹œ 30ìœ¼ë¡œ ëŠ˜ë¦´ ìˆ˜ ìˆìŒ
        "sortBy": "publishedAt",
        "from": since,
        # "searchIn": "title,description",  # í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ
        "apiKey": NEWS_API_KEY,
    }
    r = requests.get(url, params=params, timeout=20)
    r.raise_for_status()
    data = r.json()
    articles = data.get("articles", []) or []

    # 6) ë¡œê·¸ í•œ ì¤„ (Actions ë¡œê·¸ì—ì„œ ì›ì¸ íŒŒì•… ì‰¬ì›€)
    try:
        total = data.get("totalResults")
        print(f"[get_news] q='{q}' fetched={len(articles)} total={total} since={since}")
    except Exception:
        pass

    return articles

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì ìˆ˜ ê³„ì‚°(ì„ í˜¸ í‚¤ì›Œë“œ ê¸°ë°˜)
def score_article(article, keyword_weights):
    text = f"{article.get('title','')} {article.get('description','')}".lower()
    score = 0
    for kw, w in (keyword_weights or {}).items():
        k = str(kw).lower()
        if k and k in text:
            try:
                score += int(w)
            except Exception:
                score += 1
    return score

# ì œëª© ì •ê·œí™”(ì¤‘ë³µ ì œê±°ìš©)
def normalize_title(t: str) -> str:
    t = t or ""
    t = re.sub(r"[\[\](){}â€œâ€\"'â€˜â€™]", "", t)
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…”ë ˆê·¸ë¨ ì „ì†¡
def send_news(article):
    # ë²„íŠ¼: callback_dataëŠ” 'like'ë§Œ ì¨ë„ ë¨(ì›¹í›…ì€ ì œëª© íŒŒì‹± ê¸°ë°˜)
    kb = {"inline_keyboard": [[{"text": "ğŸ‘ ì¢‹ì•„ìš”", "callback_data": "like"}]]}

    title = article.get("title", "") or ""
    desc  = article.get("description", "") or ""
    link  = article.get("url", "") or ""

    # ì œëª© ë¼ì¸ì€ 'ğŸ“° 'ë¡œ ì‹œì‘ â†’ webhookì˜ ì œëª© íŒŒì„œê°€ ì•ˆì •ì ìœ¼ë¡œ ì¡ìŒ
    text = f"ğŸ“° {title}\n\n{desc}\n\n{link}"

    payload = {"chat_id": CHAT_ID, "text": text, "reply_markup": kb}
    r = requests.post(
        f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage",
        json=payload, timeout=20
    )
    if r.status_code != 200:
        print("Telegram error:", r.status_code, r.text)
    r.raise_for_status()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ë£¨í‹´
def send_daily_news():
    # 1) ìˆ˜ì§‘
    articles = get_news()

    # 2) ê³¼ê±° ì „ì†¡ ê¸°ë¡(ê¹ƒí—ˆë¸Œ) ë¡œë“œ
    sent_map, sent_sha = load_sent_articles()

    # 3) ì •ê·œí™” ê¸°ë°˜ ì¤‘ë³µ ì œê±° (URL + ì œëª©)
    uniq, seen_urls, seen_titles = [], set(), set()
    url_dup = title_dup = 0

    for a in articles:
        url = a.get("url")
        title = a.get("title")
        if not url or not title:
            continue

        nurl = normalize_url(url)
        ntit = normalize_title(title)

        if nurl in sent_map:     # ê³¼ê±°ì— ë³´ëƒ„ â†’ ìŠ¤í‚µ
            url_dup += 1
            continue
        if nurl in seen_urls:    # ê°™ì€ ì‹¤í–‰ ë‚´ ì¤‘ë³µ URL â†’ ìŠ¤í‚µ
            url_dup += 1
            continue
        if ntit in seen_titles:  # ê°™ì€ ì‹¤í–‰ ë‚´ ì œëª© ì¤‘ë³µ â†’ ìŠ¤í‚µ
            title_dup += 1
            continue

        seen_urls.add(nurl)
        seen_titles.add(ntit)
        uniq.append(a)

    # ì¤‘ë³µ ë¡œê·¸
    try:
        print(f"[dedup] kept={len(uniq)} hist={len(sent_map)} urlDup={url_dup} titleDup={title_dup}")
    except Exception:
        pass

    if not uniq:
        print("No new unique articles.")
        return

    # 4) ì¢‹ì•„ìš” í•™ìŠµ ê°€ì¤‘ì¹˜ ë°˜ì˜í•´ì„œ ì •ë ¬
    kw_weights = build_keyword_weights()
    uniq.sort(key=lambda a: score_article(a, kw_weights), reverse=True)

    # 5) ìƒìœ„ 4ê±´ ì „ì†¡
    picked = uniq[:4]
    for art in picked:
        send_news(art)

    # 6) ì „ì†¡ ê¸°ë¡ ì €ì¥ (ì •ê·œí™” URLì„ í‚¤ë¡œ ì €ì¥)
    now = datetime.utcnow().isoformat() + "Z"
    for art in picked:
        nurl = normalize_url(art.get("url",""))
        if nurl:
            sent_map[nurl] = now
    save_sent_articles(sent_map, sent_sha)

if __name__ == "__main__":
    send_daily_news()
